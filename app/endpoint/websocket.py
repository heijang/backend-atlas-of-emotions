import asyncio
from fastapi import APIRouter, WebSocket
from starlette.websockets import WebSocketDisconnect
from datetime import datetime
import os
import tempfile
import wave
from app.streaming_audio import get_streaming_stt_provider, get_sync_stt_provider
import json
from app.voice_identifier import save_user_voice_embedding_to_db, load_user_voice_embedding_to_memory, compare_voice_with_memory, get_user_uid_by_user_id
import librosa
from app.emotion_analyzer import analyze_emotions
from app.util.audio_utils import cut_wav_by_timestamps, get_storage_audio_path
from app.dao.report_dao import ReportDAO
import time

router = APIRouter()

BASE_DIR = "storage/audio"
WAV_DIR = os.path.join(BASE_DIR, "wav_chunks")
os.makedirs(WAV_DIR, exist_ok=True)

session_tempfiles = {}
session_mode = {}
user_voice_embeddings_mem = {}  # user_id: embedding
session_user_id = {}

@router.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    sid = id(websocket)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    wav_path = os.path.join(WAV_DIR, f"session_{ts}_{sid}.wav")
    temp_fd, temp_path = tempfile.mkstemp(suffix=".pcm")
    os.close(temp_fd)
    session_tempfiles[sid] = temp_path
    buffer = bytearray()
    print(f"ğŸŸ¢ ì—°ê²°ë¨: {websocket.client}")

    CHUNK_DURATION_SEC = 2.0
    SAMPLE_RATE = 16000
    BYTES_PER_SEC = SAMPLE_RATE * 2  # 16bit(2byte) * 16000
    CHUNK_SIZE = int(CHUNK_DURATION_SEC * BYTES_PER_SEC)

    try:
        while True:
            msg = await websocket.receive()
            if msg.get("type") == "websocket.receive":
                if "text" in msg:
                    try:
                        data = json.loads(msg["text"])
                        event = data.get("event")
                        if event == "register_user":
                            user_info = data.get("user_info", {})
                            user_id = user_info.get("user_id")
                            if user_id:
                                session_user_id[sid] = user_id
                            print(f"[register_user] ì‚¬ìš©ì ë“±ë¡ ìš”ì²­: {user_info}")
                            print("[ëª¨ë“œì „í™˜] register_user ì‹ í˜¸ ìˆ˜ì‹ ")
                            session_mode[sid] = "register_user"
                            await websocket.send_text(json.dumps({"event": "register_user", "status": "ok", "user_info": user_info}))
                        elif event == "audio_data":
                            user_info = data.get("user_info", {})
                            user_id = user_info.get("user_id")
                            if user_id:
                                session_user_id[sid] = user_id
                                # ìŒì„± ì„ë² ë”©ì´ ë©”ëª¨ë¦¬ì— ì—†ìœ¼ë©´ DBì—ì„œ ì¡°íšŒí•˜ì—¬ ì ì¬
                                if user_id not in user_voice_embeddings_mem:
                                    print(f"[ìŒì„± ì„ë² ë”©] user_id={user_id} ë©”ëª¨ë¦¬ì— ì—†ìŒ. DB ì¡°íšŒ ì‹œë„...")
                                    embedding = load_user_voice_embedding_to_memory(user_id)
                                    if embedding is not None:
                                        user_voice_embeddings_mem[user_id] = embedding
                                        print(f"[ìŒì„± ì„ë² ë”©] user_id={user_id} DBì—ì„œ ì¡°íšŒí•˜ì—¬ ë©”ëª¨ë¦¬ì— ì ì¬ ì™„ë£Œ.")
                                    else:
                                        print(f"[ìŒì„± ì„ë² ë”©] user_id={user_id} DBì—ë„ ì„ë² ë”© ì •ë³´ê°€ ì—†ìŒ.")
                            else:
                                print(f"[ë””ë²„ê·¸] audio_data ì´ë²¤íŠ¸ì—ì„œ user_idê°€ ì „ë‹¬ë˜ì§€ ì•ŠìŒ! sid={sid}")
                            print(f"[audio_data] ì‚¬ìš©ì ìŒì„± ë°ì´í„° ìš”ì²­: {user_info}")
                            print("[ëª¨ë“œì „í™˜] audio_data ì‹ í˜¸ ìˆ˜ì‹ ")
                            session_mode[sid] = "audio_data"
                            await websocket.send_text(json.dumps({"event": "audio_data", "status": "ok"}))
                        elif event == "login":
                            pass  # login ì´ë²¤íŠ¸ëŠ” ë” ì´ìƒ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ
                        else:
                            print(f"ì•Œ ìˆ˜ ì—†ëŠ” ì´ë²¤íŠ¸: {event}")
                    except Exception as e:
                        print(f"JSON íŒŒì‹±/ì´ë²¤íŠ¸ ì²˜ë¦¬ ì—ëŸ¬: {e}")
                elif "bytes" in msg:
                    chunk = msg["bytes"]
                    buffer.extend(chunk)
                    with open(temp_path, "ab") as f:
                        f.write(chunk)
                    # ë¶„ê¸°: ë“±ë¡ìš©/ì¼ë°˜ ì˜¤ë””ì˜¤
                    if session_mode.get(sid) == "register_user":
                        # ë“±ë¡ìš© ì˜¤ë””ì˜¤ ì²˜ë¦¬: íŒŒì¼ì— ì €ì¥ë§Œ í•˜ê³ , ì‹¤ì‹œê°„ STTëŠ” í•˜ì§€ ì•ŠìŒ
                        pass  # ë¶ˆí•„ìš”í•œ ìƒì„¸ ë¡œê¹… ì œê±°
                    else:
                        # ì¼ë°˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬ (ì‹¤ì‹œê°„ STT)
                        while len(buffer) >= CHUNK_SIZE:
                            chunk_bytes = buffer[:CHUNK_SIZE]
                            del buffer[:CHUNK_SIZE]
                            transcript = get_streaming_stt_provider().streaming(chunk_bytes)
                            if transcript:
                                # await websocket.send_text(f"{transcript}")
                                # ê°ì • ë¶„ì„ ì¶”ê°€
                                # chunk_bytesëŠ” PCM(16kHz, 16bit, mono) -> WAVë¡œ ë³€í™˜ í›„ numpy arrayë¡œ ë³€í™˜ í•„ìš”
                                import io
                                import numpy as np
                                with io.BytesIO() as wav_io:
                                    with wave.open(wav_io, 'wb') as wf:
                                        wf.setnchannels(1)
                                        wf.setsampwidth(2)
                                        wf.setframerate(16000)
                                        wf.writeframes(chunk_bytes)
                                    wav_io.seek(0)
                                    audio_array, _ = librosa.load(wav_io, sr=16000, mono=True)
                                emotion_result = analyze_emotions(transcript, audio_array)
                                # ìŒì„± ìœ ì‚¬ë„ ì‹ë³„
                                user_id = session_user_id.get(sid)
                                user_embedding = user_voice_embeddings_mem.get(user_id)
                                similarity = None
                                is_same = None
                                if user_embedding is not None:
                                    # chunk_bytesë¥¼ ì„ì‹œ wav íŒŒì¼ë¡œ ì €ì¥
                                    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav:
                                        with wave.open(temp_wav.name, 'wb') as wf:
                                            wf.setnchannels(1)
                                            wf.setsampwidth(2)
                                            wf.setframerate(16000)
                                            wf.writeframes(chunk_bytes)
                                        temp_wav_path = temp_wav.name
                                    try:
                                        is_same, similarity = compare_voice_with_memory(temp_wav_path, user_embedding, threshold=0.5)
                                    except Exception as e:
                                        print(f"[ì‹¤ì‹œê°„ ìŒì„± ì‹ë³„ ì—ëŸ¬] {e}")
                                    finally:
                                        os.remove(temp_wav_path)
                                else:
                                    print(f"[ì‹¤ì‹œê°„ ìŒì„± ì‹ë³„] user_id={user_id} ë©”ëª¨ë¦¬ ë‚´ ì„ë² ë”© ì •ë³´ ì—†ìŒ.")

                                # emotion_analysisì™€ voice_similarity ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì‘ë‹µìœ¼ë¡œ í†µí•©
                                await websocket.send_text(json.dumps({
                                    "event": "emotion_analysis",
                                    "transcript": transcript,
                                    "emotion": emotion_result,
                                    "is_same": is_same,
                                    "similarity": similarity
                                }, ensure_ascii=False))
                                print(f"[ì‹¤ì‹œê°„ STT] {transcript}")
                                print(f"[ì‹¤ì‹œê°„ ê°ì •ë¶„ì„ ê²°ê³¼] {json.dumps(emotion_result, ensure_ascii=False)}")
                                print(f"[ì‹¤ì‹œê°„ ìŒì„± ìœ ì‚¬ë„] similarity={similarity}, is_same={is_same}")
                else:
                    print("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë©”ì‹œì§€ íƒ€ì…")
    except WebSocketDisconnect:
        print(f"ğŸ”Œ ì—°ê²° í•´ì œ: {websocket.client}")
    except Exception as e:
        # ì—°ê²° í•´ì œ í›„ receiveë¡œ ì¸í•œ RuntimeErrorëŠ” ë¬´ì‹œ
        if isinstance(e, RuntimeError) and "Cannot call \"receive\" once a disconnect message has been received." in str(e):
            pass
        elif not isinstance(e, WebSocketDisconnect):
            print(f"âŒ ì—ëŸ¬: {e}")
    finally:
        temp_path = session_tempfiles.pop(sid, None)
        mode = session_mode.pop(sid, None)
        print(f"mode: {mode}")
        user_id = session_user_id.pop(sid, None)
        if mode in ("register_user", "audio_data"):
            try:
                user_info = data.get("user_info", {}) if 'data' in locals() else {}
                if not user_id:
                    user_id = user_info.get("user_id")
            except Exception as e:
                print(f"[{mode}] user_info ì¶”ì¶œ ì—ëŸ¬: {e}")
        if temp_path and os.path.exists(temp_path):
            with open(temp_path, "rb") as f:
                pcm_bytes = f.read()
            # WAV í—¤ë” ë¶™ì—¬ì„œ ìƒˆ íŒŒì¼ë¡œ ì €ì¥
            with wave.open(wav_path, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(16000)
                wf.writeframes(pcm_bytes)
            os.remove(temp_path)
            if mode == "register_user":
                print(f"[register_user] ë“±ë¡ìš© ì˜¤ë””ì˜¤ WAV íŒŒì¼ ê²½ë¡œ: {wav_path}")
                # ìŒì„± ì„ë² ë”© ë° DB ì €ì¥
                if user_id:
                    try:
                        print(f"[register_user] ìŒì„± ì„ë² ë”© ë° DB ì €ì¥ ì‹œì‘: user_id={user_id}")
                        user_uid = get_user_uid_by_user_id(user_id)
                        if user_uid is not None:
                            embedding = save_user_voice_embedding_to_db(user_uid, wav_path)
                            print(f"[register_user] ìŒì„± ì„ë² ë”© ë° DB ì €ì¥ ì™„ë£Œ: user_uid={user_uid}")
                            # ë©”ëª¨ë¦¬ì—ë„ ì ì¬
                            user_voice_embeddings_mem[user_id] = embedding
                            print(f"[register_user] user_id={user_id} ì„ë² ë”©ì„ ë©”ëª¨ë¦¬ì— ì ì¬ ì™„ë£Œ.")
                        else:
                            print(f"[register_user] user_uidë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: user_id={user_id}")
                    except Exception as e:
                        print(f"[register_user] ìŒì„± ì„ë² ë”©/DB ì €ì¥ ì—ëŸ¬: {e}")
            elif mode == "audio_data":
                with open(wav_path, "rb") as f:
                    full_audio = f.read()
                # Clova ë¶„ì„ ì‹œê°„ ì¸¡ì •
                clova_start = time.time()
                print(f"[Clova ë¶„ì„] ìš”ì²­ ì‹œì‘: {clova_start}")
                final_result = get_sync_stt_provider().sync(full_audio)
                clova_end = time.time()
                print(f"[Clova ë¶„ì„] ìš”ì²­ ì¢…ë£Œ: {clova_end}, ì†Œìš”ì‹œê°„: {clova_end - clova_start:.2f}ì´ˆ")
                if isinstance(final_result, list):
                    print("[ìµœì¢… STT - Clova diarization ê²°ê³¼]")
                    # DB ì €ì¥ ë¡œì§ ì¶”ê°€
                    user_id_for_db = user_id if user_id else "test_user"
                    user_uid_for_db = get_user_uid_by_user_id(user_id_for_db)
                    report_dao = ReportDAO()
                    master_uid = None
                    try:
                        # ë§ˆìŠ¤í„° row ìƒì„± (topicì€ None ë˜ëŠ” ìë™ ìƒì„±)
                        master_uid = report_dao.insert_conversation_master(user_uid_for_db, topic=None)
                        print(f"[DB] user_conversation_master ì €ì¥: master_uid={master_uid}")
                    except Exception as e:
                        print(f"[DB] master ì €ì¥ ì—ëŸ¬: {e}")
                    user_embedding = user_voice_embeddings_mem.get(user_id)
                    if user_embedding is None:
                        print(f"[ìŒì„± ì‹ë³„] user_id={user_id} ë©”ëª¨ë¦¬ ë‚´ ì„ë² ë”© ì •ë³´ ì—†ìŒ.")
                    segment_timestamps = []
                    for seg in final_result:
                        start = seg.get('start')
                        end = seg.get('end')
                        if start is not None and end is not None:
                            # ms â†’ ì´ˆ ë³€í™˜
                            start_sec = start / 1000
                            end_sec = end / 1000
                            segment_timestamps.append((start_sec, end_sec))
                    segment_dir = get_storage_audio_path(f"segments/{ts}_{sid}")
                    segment_files = cut_wav_by_timestamps(wav_path, segment_timestamps, segment_dir)
                    print(f"[ë¬¸ì¥ë³„ ì˜¤ë””ì˜¤ ì»·íŒ… ê²½ë¡œ] {segment_files}")
                    import json as _json
                    for i, seg in enumerate(final_result):
                        print(f"[Segment {i+1}] Speaker: {seg.get('speaker')} | Text: {seg.get('text')}")
                        try:
                            seg_wav_path = segment_files[i] if i < len(segment_files) else wav_path
                            is_same, similarity = compare_voice_with_memory(seg_wav_path, user_embedding, threshold=0.75)
                            print(f"[ìŒì„± ì‹ë³„] Segment {i+1} | ìœ ì‚¬ë„: {similarity:.4f} | ë™ì¼ì¸: {is_same} | íŒŒì¼: {seg_wav_path}")
                            # Gemini ê°ì •ë¶„ì„ ì‹œê°„ ì¸¡ì •
                            gemini_start = time.time()
                            print(f"[Gemini ë¶„ì„] ìš”ì²­ ì‹œì‘: {gemini_start}")
                            # DBì— detail ì €ì¥
                            if master_uid:
                                try:
                                    # emotion_result: ì „ì²´ ê°ì •ë¶„ì„ ê²°ê³¼
                                    # dominant_emotion: audio.scoresì—ì„œ ê°€ì¥ ë†’ì€ ê°’ì˜ key
                                    emotion_result = seg.get('emotion_result')
                                    if not emotion_result:
                                        # backward compatibility: segì— emotion_scoreë§Œ ìˆì„ ê²½ìš°
                                        emotion_result = {}
                                    else:
                                        # ì´ë¯¸ emotion_resultê°€ dictë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                        pass
                                    # dominant_emotion ì¶”ì¶œ
                                    dominant_emotion = None
                                    if emotion_result and 'audio' in emotion_result and 'scores' in emotion_result['audio']:
                                        scores = emotion_result['audio']['scores']
                                        if isinstance(scores, dict):
                                            # ê°’ì´ strì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ float ë³€í™˜
                                            try:
                                                dominant_emotion = max(scores, key=lambda k: float(scores[k]))
                                            except Exception:
                                                dominant_emotion = None
                                    report_dao.insert_conversation_detail(
                                        master_uid=master_uid,
                                        sentence=seg.get('text'),
                                        speaker=str(seg.get('speaker', {}).get('label')) if isinstance(seg.get('speaker'), dict) else str(seg.get('speaker')),
                                        emotion_result=_json.dumps(emotion_result, ensure_ascii=False),
                                        dominant_emotion=dominant_emotion,
                                        start_ms=seg.get('start'),
                                        end_ms=seg.get('end')
                                    )
                                    gemini_end = time.time()
                                    print(f"[Gemini ë¶„ì„] ìš”ì²­ ì¢…ë£Œ: {gemini_end}, ì†Œìš”ì‹œê°„: {gemini_end - gemini_start:.2f}ì´ˆ")
                                    print(f"[DB] user_conversation_detail ì €ì¥: master_uid={master_uid}, seg_idx={i}")
                                except Exception as e:
                                    print(f"[DB] detail ì €ì¥ ì—ëŸ¬: {e}")
                        except Exception as e:
                            print(f"[ìŒì„± ì‹ë³„] Segment {i+1} | ì—ëŸ¬: {e}")
                    if master_uid:
                        try:
                            report_dao.update_master_audio_path(master_uid, wav_path)
                            print(f"[DB] user_conversation_master.audio_path ì—…ë°ì´íŠ¸: master_uid={master_uid}")
                        except Exception as e:
                            print(f"[DB] master.audio_path ì—…ë°ì´íŠ¸ ì—ëŸ¬: {e}")

                    try:
                        report_dao.close()
                    except Exception:
                        pass
                else:
                    print(f"[ìµœì¢… STT] {final_result}") 