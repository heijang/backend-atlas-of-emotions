# Standard library imports
import io
import json
import os
import tempfile
import time
import wave
from datetime import datetime

# Third-party imports
import librosa
from fastapi import APIRouter, WebSocket
from starlette.websockets import WebSocketDisconnect

# Local application imports
from app.dao.user_conversation_dao import UserConversationDAO
from app.providers.gemini_client import analyze_emotions
from app.providers.stt_provider import get_streaming_stt_provider, get_sync_stt_provider
from app.utils.audio_utils import cut_wav_by_timestamps, get_storage_audio_path
from app.services.user_services import user_service
from app.services.user_voice_service import user_voice_service

router = APIRouter()

BASE_DIR = "storage/audio"
WAV_DIR = os.path.join(BASE_DIR, "wav_chunks")
os.makedirs(WAV_DIR, exist_ok=True)

session_tempfiles = {}
session_mode = {}
user_voice_embeddings_mem = {}  # user_id: embedding
session_user_id = {}

@router.websocket("/ws/analyze")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    sid = id(websocket)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
    wav_path = os.path.join(WAV_DIR, f"session_{ts}_{sid}.wav")
    temp_fd, temp_path = tempfile.mkstemp(suffix=".pcm")
    os.close(temp_fd)
    session_tempfiles[sid] = temp_path
    buffer = bytearray()
    full_audio_buffer = bytearray()
    print(f"ğŸŸ¢ ì—°ê²°ë¨: {websocket.client}")

    CHUNK_DURATION_SEC = 2.0
    SAMPLE_RATE = 16000
    BYTES_PER_SEC = SAMPLE_RATE * 2  # 16bit(2byte) * 16000
    CHUNK_SIZE = int(CHUNK_DURATION_SEC * BYTES_PER_SEC)

    report_dao = UserConversationDAO()
    master_uid = None

    try:
        setup_data = await websocket.receive_json()
        event = setup_data.get("event")
        if event == "send_conversation":
            user_info = setup_data.get("user_info", {})
            user_id = user_info.get("user_id")
            if user_id:
                session_user_id[sid] = user_id
                if user_id not in user_voice_embeddings_mem:
                    print(f"[ìŒì„± ì„ë² ë”©] user_id={user_id} ë©”ëª¨ë¦¬ì— ì—†ìŒ. DB ì¡°íšŒ ì‹œë„...")
                    user_uid = user_service.get_user_uid_by_user_id(user_id)
                    if user_uid:
                        embedding = user_voice_service.get_user_voice_embedding(user_uid)
                        if embedding is not None:
                            user_voice_embeddings_mem[user_id] = embedding
                            print(f"[ìŒì„± ì„ë² ë”©] user_id={user_id} DBì—ì„œ ì¡°íšŒí•˜ì—¬ ë©”ëª¨ë¦¬ì— ì ì¬ ì™„ë£Œ.")
                        else:
                            print(f"[ìŒì„± ì„ë² ë”©] user_id={user_id} DBì—ë„ ì„ë² ë”© ì •ë³´ê°€ ì—†ìŒ.")
                    else:
                        print(f"[ìŒì„± ì„ë² ë”©] user_id={user_id} ì— í•´ë‹¹í•˜ëŠ” user_uid ì—†ìŒ.")
            session_mode[sid] = event
            await websocket.send_text(json.dumps({"event": "send_conversation", "status": "ok"}))
        else:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” ì´ë²¤íŠ¸: {event}")
            await websocket.close(code=1008)
            return

        while True:
            chunk = await websocket.receive_bytes()
            buffer.extend(chunk)
            full_audio_buffer.extend(chunk)

            while len(buffer) >= CHUNK_SIZE:
                chunk_bytes = buffer[:CHUNK_SIZE]
                del buffer[:CHUNK_SIZE]
                transcript = get_streaming_stt_provider().streaming(chunk_bytes)
                if transcript:
                    # ê°ì • ë¶„ì„ ì¶”ê°€
                    # chunk_bytesëŠ” PCM(16kHz, 16bit, mono) -> WAVë¡œ ë³€í™˜ í›„ numpy arrayë¡œ ë³€í™˜ í•„ìš”
                    with io.BytesIO() as wav_io:
                        with wave.open(wav_io, 'wb') as wf:
                            wf.setnchannels(1)
                            wf.setsampwidth(2)
                            wf.setframerate(16000)
                            wf.writeframes(chunk_bytes)
                        wav_io.seek(0)
                        audio_array, _ = librosa.load(wav_io, sr=16000, mono=True)
                    emotion_result = analyze_emotions(transcript, audio_array)
                    # ìŒì„± ìœ ì‚¬ë„ ì‹ë³„
                    user_id = session_user_id.get(sid)
                    user_embedding = user_voice_embeddings_mem.get(user_id)
                    similarity = None
                    is_same = None
                    if user_embedding is not None:
                        # chunk_bytesë¥¼ ì„ì‹œ wav íŒŒì¼ë¡œ ì €ì¥
                        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_wav:
                            with wave.open(temp_wav.name, 'wb') as wf:
                                wf.setnchannels(1)
                                wf.setsampwidth(2)
                                wf.setframerate(16000)
                                wf.writeframes(chunk_bytes)
                            temp_wav_path = temp_wav.name
                        try:
                            is_same, similarity = user_voice_service.compare_voice(temp_wav_path, user_embedding, threshold=0.5)
                        except Exception as e:
                            print(f"[ì‹¤ì‹œê°„ ìŒì„± ì‹ë³„ ì—ëŸ¬] {e}")
                        finally:
                            os.remove(temp_wav_path)
                    else:
                        print(f"[ì‹¤ì‹œê°„ ìŒì„± ì‹ë³„] user_id={user_id} ë©”ëª¨ë¦¬ ë‚´ ì„ë² ë”© ì •ë³´ ì—†ìŒ.")

                    # emotion_analysisì™€ voice_similarity ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì‘ë‹µìœ¼ë¡œ í†µí•©
                    await websocket.send_text(json.dumps({
                        "event": "emotion_analysis",
                        "transcript": transcript,
                        "emotion": emotion_result,
                        "is_same": is_same,
                        "similarity": similarity
                    }, ensure_ascii=False))
                    print(f"[ì‹¤ì‹œê°„ STT] {transcript}")
                    print(f"[ì‹¤ì‹œê°„ ê°ì •ë¶„ì„ ê²°ê³¼] {json.dumps(emotion_result, ensure_ascii=False)}")
                    print(f"[ì‹¤ì‹œê°„ ìŒì„± ìœ ì‚¬ë„] similarity={similarity}, is_same={is_same}")

    except WebSocketDisconnect:
        print(f"ğŸ”Œ ì—°ê²° í•´ì œ (ë°ì´í„° ìˆ˜ì‹  ì™„ë£Œ)")
    except Exception as e:
        if not isinstance(e, WebSocketDisconnect):
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
    finally:
        print(f"ğŸ”Œ í›„ì²˜ë¦¬ ì‹œì‘. ìˆ˜ì‹ ëœ ì´ ë°ì´í„° í¬ê¸°: {len(full_audio_buffer)} bytes")
        mode = session_mode.pop(sid, None)
        user_id = session_user_id.pop(sid, None)

        if mode == "send_conversation" and len(full_audio_buffer) > 0:
            # ì „ì²´ PCM ë°ì´í„°ë¥¼ WAV íŒŒì¼ë¡œ ì €ì¥
            with wave.open(wav_path, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(16000)
                wf.writeframes(full_audio_buffer)
            
            # í›„ì²˜ë¦¬: STT ë¶„ì„ ë° DB ì €ì¥
            with open(wav_path, "rb") as f:
                full_audio = f.read()
            # Clova STT ë¶„ì„
            clova_start = time.time()
            print(f"[Clova ë¶„ì„] ìš”ì²­ ì‹œì‘: {clova_start}")
            final_result = get_sync_stt_provider().sync(full_audio)
            clova_end = time.time()
            print(f"[Clova ë¶„ì„] ìš”ì²­ ì¢…ë£Œ: {clova_end}, ì†Œìš”ì‹œê°„: {clova_end - clova_start:.2f}ì´ˆ")
            if isinstance(final_result, list):
                print("[ìµœì¢… STT - Clova diarization ê²°ê³¼]")
                # DB ì €ì¥ ë¡œì§ ì¶”ê°€
                user_id_for_db = user_id if user_id else "test_user"
                user_uid_for_db = user_service.get_user_uid_by_user_id(user_id_for_db)
                master_uid = report_dao.insert_conversation_master(user_uid_for_db, topic=None)
                print(f"[DB] user_conversation_master ì €ì¥: master_uid={master_uid}")
                user_embedding = user_voice_embeddings_mem.get(user_id)
                if user_embedding is None:
                    print(f"[ìŒì„± ì‹ë³„] user_id={user_id} ë©”ëª¨ë¦¬ ë‚´ ì„ë² ë”© ì •ë³´ ì—†ìŒ.")
                segment_timestamps = [(seg.get('start') / 1000, seg.get('end') / 1000) for seg in final_result]
                segment_dir = get_storage_audio_path(f"segments/{ts}_{sid}")
                segment_files = cut_wav_by_timestamps(wav_path, segment_timestamps, segment_dir)
                print(f"[ë¬¸ì¥ë³„ ì˜¤ë””ì˜¤ ì»·íŒ… ê²½ë¡œ] {segment_files}")
                for i, seg in enumerate(final_result):
                    print(f"[Segment {i+1}] Speaker: {seg.get('speaker')} | Text: {seg.get('text')}")
                    try:
                        seg_wav_path = segment_files[i] if i < len(segment_files) else wav_path
                        is_same, similarity = user_voice_service.compare_voice(seg_wav_path, user_embedding, threshold=0.75)
                        print(f"[ìŒì„± ì‹ë³„] Segment {i+1} | ìœ ì‚¬ë„: {similarity:.4f} | ë™ì¼ì¸: {is_same} | íŒŒì¼: {seg_wav_path}")
                        # Gemini ê°ì •ë¶„ì„ ì‹œê°„ ì¸¡ì •
                        gemini_start = time.time()
                        print(f"[Gemini ë¶„ì„] ìš”ì²­ ì‹œì‘: {gemini_start}")
                        # DBì— detail ì €ì¥
                        if master_uid:
                            try:
                                # emotion_result: ì „ì²´ ê°ì •ë¶„ì„ ê²°ê³¼
                                # dominant_emotion: audio.scoresì—ì„œ ê°€ì¥ ë†’ì€ ê°’ì˜ key
                                emotion_result = seg.get('emotion_result')
                                if not emotion_result:
                                    # backward compatibility: segì— emotion_scoreë§Œ ìˆì„ ê²½ìš°
                                    emotion_result = {}
                                else:
                                    # ì´ë¯¸ emotion_resultê°€ dictë¡œ ë“¤ì–´ì˜¨ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                    pass
                                # dominant_emotion ì¶”ì¶œ
                                dominant_emotion = None
                                if emotion_result and 'audio' in emotion_result and 'scores' in emotion_result['audio']:
                                    scores = emotion_result['audio']['scores']
                                    if isinstance(scores, dict):
                                        # ê°’ì´ strì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ float ë³€í™˜
                                        try:
                                            dominant_emotion = max(scores, key=lambda k: float(scores[k]))
                                        except Exception:
                                            dominant_emotion = None
                                report_dao.insert_conversation_detail(
                                    master_uid=master_uid,
                                    sentence=seg.get('text'),
                                    speaker=str(seg.get('speaker', {}).get('label')) if isinstance(seg.get('speaker'), dict) else str(seg.get('speaker')),
                                    emotion_result=json.dumps(emotion_result, ensure_ascii=False),
                                    dominant_emotion=dominant_emotion,
                                    start_ms=seg.get('start'),
                                    end_ms=seg.get('end')
                                )
                                gemini_end = time.time()
                                print(f"[Gemini ë¶„ì„] ìš”ì²­ ì¢…ë£Œ: {gemini_end}, ì†Œìš”ì‹œê°„: {gemini_end - gemini_start:.2f}ì´ˆ")
                                print(f"[DB] user_conversation_detail ì €ì¥: master_uid={master_uid}, seg_idx={i}")
                            except Exception as e:
                                print(f"[DB] detail ì €ì¥ ì—ëŸ¬: {e}")
                    except Exception as e:
                        print(f"[ìŒì„± ì‹ë³„] Segment {i+1} | ì—ëŸ¬: {e}")
                if master_uid:
                    try:
                        report_dao.update_master_audio_path(master_uid, wav_path)
                        print(f"[DB] user_conversation_master.audio_path ì—…ë°ì´íŠ¸: master_uid={master_uid}")
                    except Exception as e:
                        print(f"[DB] master.audio_path ì—…ë°ì´íŠ¸ ì—ëŸ¬: {e}")

                try:
                    report_dao.close()
                except Exception:
                    pass
            else:
                print(f"[ìµœì¢… STT] {final_result}") 
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        temp_path = session_tempfiles.pop(sid, None)
        if temp_path and os.path.exists(temp_path):
            os.remove(temp_path)
        
        print("ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ.")